{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import impute\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read file and make initial replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = pd.read_csv('./data/application_train.csv')\n",
    "application_test = pd.read_csv('./data/application_test.csv')\n",
    "\n",
    "application_train.replace('XNA', np.NaN, inplace = True)\n",
    "application_train['age_yrs'] = np.negative(application_train['DAYS_BIRTH'])/365\n",
    "application_train['yrs_emp'] = np.negative(application_train['DAYS_EMPLOYED'])/365\n",
    "application_train['yrs_registration'] = np.negative(application_train['DAYS_REGISTRATION'])/365\n",
    "application_train['yrs_id_publish'] = np.negative(application_train['DAYS_ID_PUBLISH'])/365\n",
    "application_train['yrs_last_phone_change'] = np.negative(application_train['DAYS_LAST_PHONE_CHANGE'])/365\n",
    "application_train = application_train.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE'], axis = 1)\n",
    "\n",
    "application_test.replace('XNA', np.NaN, inplace = True)\n",
    "application_test['age_yrs'] = np.negative(application_test['DAYS_BIRTH'])/365\n",
    "application_test['yrs_emp'] = np.negative(application_test['DAYS_EMPLOYED'])/365\n",
    "application_test['yrs_registration'] = np.negative(application_test['DAYS_REGISTRATION'])/365\n",
    "application_test['yrs_id_publish'] = np.negative(application_test['DAYS_ID_PUBLISH'])/365\n",
    "application_test['yrs_last_phone_change'] = np.negative(application_test['DAYS_LAST_PHONE_CHANGE'])/365\n",
    "application_test = application_test.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE'], axis = 1)\n",
    "\n",
    "# imputing self-proclaimed-income anomalies\n",
    "inc_anomalies = application_train[(application_train.AMT_INCOME_TOTAL - np.mean(application_train.AMT_INCOME_TOTAL))/np.std(application_train.AMT_INCOME_TOTAL) > 3]\n",
    "application_train.AMT_INCOME_TOTAL.replace(inc_anomalies.AMT_INCOME_TOTAL.values, np.mean(application_train.AMT_INCOME_TOTAL), inplace = True)\n",
    "\n",
    "# replacing one aberration with mean\n",
    "application_train.OBS_30_CNT_SOCIAL_CIRCLE.replace(max(application_train.OBS_30_CNT_SOCIAL_CIRCLE), np.mean(application_train.OBS_30_CNT_SOCIAL_CIRCLE), inplace = True)\n",
    "application_train.DEF_30_CNT_SOCIAL_CIRCLE.replace(max(application_train.DEF_30_CNT_SOCIAL_CIRCLE), np.mean(application_train.DEF_30_CNT_SOCIAL_CIRCLE), inplace = True)\n",
    "application_train.OBS_60_CNT_SOCIAL_CIRCLE.replace(max(application_train.OBS_60_CNT_SOCIAL_CIRCLE), np.mean(application_train.OBS_60_CNT_SOCIAL_CIRCLE), inplace = True)\n",
    "application_train.DEF_60_CNT_SOCIAL_CIRCLE.replace(max(application_train.DEF_60_CNT_SOCIAL_CIRCLE), np.mean(application_train.DEF_60_CNT_SOCIAL_CIRCLE), inplace = True)\n",
    "\n",
    "application_train1 = application_train.copy()\n",
    "application_test1 = application_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values imputation as per best strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables having missing values\n",
    "cat_var = application_train.dtypes[application_train.dtypes == 'object'].index\n",
    "\n",
    "# Numerical variables having missing values\n",
    "num_var = application_train.dtypes[application_train.dtypes != 'object'].index[2:]\n",
    "\n",
    "# IMPUTATION of missing values\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'constant', fill_value = 'xan')\n",
    "application_train[cat_var] = imp.fit_transform(application_train[cat_var])\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'constant', fill_value = 'xan')\n",
    "application_test[cat_var] = imp.fit_transform(application_test[cat_var])\n",
    "\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'median')\n",
    "application_train[num_var] = imp.fit_transform(application_train[num_var])\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'median')\n",
    "application_test[num_var] = imp.fit_transform(application_test[num_var])\n",
    "\n",
    "application_train2 = application_train.copy()\n",
    "application_test2 = application_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummification and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMMIFICATION of categorical variables\n",
    "logistic_train = application_train.copy()\n",
    "dummy_train = logistic_train[cat_var]\n",
    "dummy_train['code'] = 1\n",
    "\n",
    "logistic_test = application_test.copy()\n",
    "dummy_test = logistic_test[cat_var]\n",
    "dummy_test['code'] = 0\n",
    "\n",
    "dummy_joined = pd.concat([dummy_train, dummy_test])\n",
    "dummy_joined = pd.get_dummies(dummy_joined, drop_first = True)\n",
    "\n",
    "dummy_train = dummy_joined[dummy_joined.code == 1]\n",
    "logistic_train = logistic_train.drop(cat_var, axis=1)\n",
    "logistic_train = pd.concat([logistic_train, dummy_train], axis = 1)\n",
    "logistic_train = logistic_train.drop('code', axis = 1)\n",
    "\n",
    "dummy_test = dummy_joined[dummy_joined.code == 0]\n",
    "logistic_test = logistic_test.drop(cat_var, axis=1)\n",
    "logistic_test = pd.concat([logistic_test, dummy_test], axis = 1)\n",
    "logistic_test = logistic_test.drop('code', axis = 1)\n",
    "\n",
    "\n",
    "# STANDARDIZATION of variables\n",
    "logistic_train.iloc[:,2:] = logistic_train.iloc[:,2:].apply(lambda x: (x - np.mean(x)) / np.std(x), axis = 0)\n",
    "\n",
    "logistic_test.iloc[:,1:] = logistic_test.iloc[:,1:].apply(lambda x: (x - np.mean(x)) / np.std(x), axis = 0)\n",
    "logistic_test[['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12',\n",
    "       'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',\n",
    "       'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19',\n",
    "       'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'CODE_GENDER_xan',\n",
    "       'NAME_INCOME_TYPE_Maternity leave', 'NAME_FAMILY_STATUS_Unknown']] = 0\n",
    "\n",
    "application_train3 = logistic_train.copy()\n",
    "application_test3 = logistic_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "one_index = logistic_train[logistic_train.TARGET == 1].index\n",
    "zero_index = logistic_train[logistic_train.TARGET == 0].index\n",
    "\n",
    "trainindex1 = np.random.choice(one_index, size = int(0.7*one_index.shape[0]), replace = False)\n",
    "trainindex0 = np.random.choice(zero_index, size = int(0.7*zero_index.shape[0]), replace = False)\n",
    "trainindex = np.concatenate([trainindex1, trainindex0])\n",
    "testindex = np.delete(logistic_train.index, trainindex)\n",
    "\n",
    "fit_df = logistic_train.iloc[trainindex]\n",
    "validation_df = logistic_train.iloc[testindex]\n",
    "\n",
    "X_fit = fit_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_fit = fit_df['TARGET']\n",
    "X_validation = validation_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_validation = validation_df['TARGET']\n",
    "\n",
    "# Model fit and prediction\n",
    "roc_fit = []\n",
    "roc_validation = []\n",
    "for c in [0.01, 1, 1000]:\n",
    "    logitmodel = linear_model.LogisticRegression(C = c, class_weight = 'balanced')\n",
    "    logitmodel.fit(X_fit, y_fit)\n",
    "\n",
    "    y_pred_fit = logitmodel.predict(X_fit)\n",
    "    roc_fit.append(metrics.roc_auc_score(y_fit, y_pred_fit))\n",
    "\n",
    "    y_pred_validation = logitmodel.predict(X_validation)\n",
    "    roc_validation.append(metrics.roc_auc_score(y_validation, y_pred_validation))\n",
    "print(roc_fit)\n",
    "print(roc_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting TARGET in test as per best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = logistic_train.iloc[:, 2:]\n",
    "y_train = logistic_train.TARGET\n",
    "X_test = logistic_test.iloc[:, 1:]\n",
    "\n",
    "# Model fit and prediction\n",
    "logitmodel = linear_model.LogisticRegression(C = 1000, class_weight = 'balanced')\n",
    "logitmodel.fit(X_train, y_train)\n",
    "y_pred = logitmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_log = pd.DataFrame({'SK_ID_CURR': logistic_test.iloc[:, 0], 'TARGET': y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_log.to_csv('./soln/sol_log.csv', index=False)     # gave kaggle score of 0.68185"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
