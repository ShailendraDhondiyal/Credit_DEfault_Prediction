{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import impute\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_mod = pd.read_csv('./data/app_train_mod.csv')\n",
    "app_test_mod = pd.read_csv('./data/app_test_mod.csv')\n",
    "bureau = pd.read_csv('./data/bureau.csv')\n",
    "bureau_balance = pd.read_csv('./data/bureau_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_train = pd.merge(app_train_mod[['SK_ID_CURR']], bureau, on = 'SK_ID_CURR', how = 'inner')\n",
    "bureau_test = pd.merge(app_test_mod[['SK_ID_CURR']], bureau, on = 'SK_ID_CURR', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying AMT_CREDIT_SUM_DEBT for anomalies (high -ve values): These are mainly in cards where a few entries have \n",
    "# taken unused credit limit as -ve debt\n",
    "bureau_train['AMT_CREDIT_SUM_DEBT'][-bureau_train['AMT_CREDIT_SUM_DEBT'] > .05*bureau_train['AMT_CREDIT_SUM']] = 0\n",
    "bureau_test['AMT_CREDIT_SUM_DEBT'][-bureau_test['AMT_CREDIT_SUM_DEBT'] > .05*bureau_test['AMT_CREDIT_SUM']] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bureau.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding new modified features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total of debt, overdue amount, count of credits prolonged and annuity amount per SK_ID_CURR\n",
    "bureau_train_new = bureau.groupby('SK_ID_CURR', as_index = False).agg({'AMT_CREDIT_SUM_DEBT': 'sum', 'AMT_CREDIT_SUM_OVERDUE': 'sum',\n",
    "                              'CNT_CREDIT_PROLONG': 'sum', 'AMT_ANNUITY': 'sum',  \n",
    "                                'DAYS_CREDIT': ['mean', 'max'], 'CREDIT_DAY_OVERDUE': ['mean', 'max']})\n",
    "bureau_train_new.columns = ['SK_ID_CURR', 'AMT_CREDIT_SUM_DEBT_TOT', 'AMT_CREDIT_SUM_OVERDUE_TOT',\n",
    "       'CNT_CREDIT_PROLONG_TOT', 'AMT_ANNUITY_TOT', 'DAYS_CREDIT_mean', 'DAYS_CREDIT_max',\n",
    "                          'CREDIT_DAY_OVERDUE_mean', 'CREDIT_DAY_OVERDUE_max']\n",
    "\n",
    "# Count of active bureau loans per SK_ID_CURR\n",
    "bureau_train_new1 = bureau[bureau.CREDIT_ACTIVE == 'Active'].groupby('SK_ID_CURR', as_index = False).count()[['SK_ID_CURR','CREDIT_ACTIVE']]\n",
    "bureau_train_new1.columns = ['SK_ID_CURR', 'CNT_ACTIVE_LOAN']\n",
    "\n",
    "# Count of Bad debt loans per SK_ID_CURR\n",
    "bureau_train_new2 = bureau[bureau.CREDIT_ACTIVE == 'Bad debt'].groupby('SK_ID_CURR', as_index = False).count()[['SK_ID_CURR','CREDIT_ACTIVE']]\n",
    "bureau_train_new2.columns = ['SK_ID_CURR', 'CNT_BAD_DEBT']\n",
    "\n",
    "# Count of bureau loans applied in last one year per SK_ID_CURR\n",
    "bureau_train_new3 = bureau[bureau.DAYS_CREDIT >= -365].groupby('SK_ID_CURR', as_index = False).count()[['SK_ID_CURR','DAYS_CREDIT']]\n",
    "bureau_train_new3.columns = ['SK_ID_CURR', 'CNT_APP_1YR']\n",
    "\n",
    "# Merging all the new features\n",
    "x = pd.merge(bureau_train_new1, bureau_train_new2, how='outer', on = 'SK_ID_CURR')\n",
    "x = pd.merge(x, bureau_train_new3, how='outer', on = 'SK_ID_CURR')\n",
    "bureau_train_new = pd.merge(bureau_train_new, x, how='outer', on = 'SK_ID_CURR')\n",
    "bureau_train_new.iloc[:, 1:] = bureau_train_new.iloc[:, 1:].apply(lambda x: (x - np.mean(x)) / np.std(x), axis = 0)\n",
    "bureau_train_new = bureau_train_new.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total of debt, overdue amount, count of credits prolonged and annuity amount per SK_ID_CURR\n",
    "bureau_test_new = bureau.groupby('SK_ID_CURR', as_index = False).agg({'AMT_CREDIT_SUM_DEBT': 'sum', 'AMT_CREDIT_SUM_OVERDUE': 'sum',\n",
    "                              'CNT_CREDIT_PROLONG': 'sum', 'AMT_ANNUITY': 'sum',  \n",
    "                                'DAYS_CREDIT': ['mean', 'max'], 'CREDIT_DAY_OVERDUE': ['mean', 'max']})\n",
    "bureau_test_new.columns = ['SK_ID_CURR', 'AMT_CREDIT_SUM_DEBT_TOT', 'AMT_CREDIT_SUM_OVERDUE_TOT',\n",
    "       'CNT_CREDIT_PROLONG_TOT', 'AMT_ANNUITY_TOT', 'DAYS_CREDIT_mean', 'DAYS_CREDIT_max',\n",
    "                          'CREDIT_DAY_OVERDUE_mean', 'CREDIT_DAY_OVERDUE_max']\n",
    "\n",
    "# Count of active bureau loans per SK_ID_CURR\n",
    "bureau_test_new1 = bureau[bureau.CREDIT_ACTIVE == 'Active'].groupby('SK_ID_CURR', as_index = False).count()[['SK_ID_CURR','CREDIT_ACTIVE']]\n",
    "bureau_test_new1.columns = ['SK_ID_CURR', 'CNT_ACTIVE_LOAN']\n",
    "\n",
    "# Count of Bad debt loans per SK_ID_CURR\n",
    "bureau_test_new2 = bureau[bureau.CREDIT_ACTIVE == 'Bad debt'].groupby('SK_ID_CURR', as_index = False).count()[['SK_ID_CURR','CREDIT_ACTIVE']]\n",
    "bureau_test_new2.columns = ['SK_ID_CURR', 'CNT_BAD_DEBT']\n",
    "\n",
    "# Count of bureau loans applied in last one year per SK_ID_CURR\n",
    "bureau_test_new3 = bureau[bureau.DAYS_CREDIT >= -365].groupby('SK_ID_CURR', as_index = False).count()[['SK_ID_CURR','DAYS_CREDIT']]\n",
    "bureau_test_new3.columns = ['SK_ID_CURR', 'CNT_APP_1YR']\n",
    "\n",
    "# Merging all the new features\n",
    "x = pd.merge(bureau_test_new1, bureau_test_new2, how='outer', on = 'SK_ID_CURR')\n",
    "x = pd.merge(x, bureau_test_new3, how='outer', on = 'SK_ID_CURR')\n",
    "bureau_test_new = pd.merge(bureau_test_new, x, how='outer', on = 'SK_ID_CURR')\n",
    "bureau_test_new.iloc[:, 1:] = bureau_test_new.iloc[:, 1:].apply(lambda x: (x - np.mean(x)) / np.std(x), axis = 0)\n",
    "bureau_test_new = bureau_test_new.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bureau_balance.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No. of times a SK_ID_BUREAU had DPD 1, 2, 3, 4, 5 in last one year\n",
    "balance12 = bureau_balance[(bureau_balance.MONTHS_BALANCE >= -12) & ~(bureau_balance.STATUS.isin(['0', 'C', 'X']))]\n",
    "\n",
    "DPD1_1YR = balance12[bureau_balance.STATUS == '1'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD1_1YR.columns = ('SK_ID_BUREAU', 'DPD1_1YR')\n",
    "\n",
    "DPD2_1YR = balance12[bureau_balance.STATUS == '2'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD2_1YR.columns = ('SK_ID_BUREAU', 'DPD2_1YR')\n",
    "\n",
    "DPD3_1YR = balance12[bureau_balance.STATUS == '3'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD3_1YR.columns = ('SK_ID_BUREAU', 'DPD3_1YR')\n",
    "\n",
    "DPD4_1YR = balance12[bureau_balance.STATUS == '4'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD4_1YR.columns = ('SK_ID_BUREAU', 'DPD4_1YR')\n",
    "\n",
    "DPD5_1YR = balance12[bureau_balance.STATUS == '5'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD5_1YR.columns = ('SK_ID_BUREAU', 'DPD5_1YR')\n",
    "\n",
    "DPDany_1YR = balance12[bureau_balance.STATUS.isin(['1','2','3','4','5'])].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPDany_1YR.columns = ('SK_ID_BUREAU', 'DPDany_1YR')\n",
    "\n",
    "DPD_1YR = pd.merge(DPD1_1YR, DPD2_1YR, how='outer', on = 'SK_ID_BUREAU')\n",
    "DPD_1YR = pd.merge(DPD_1YR, DPD3_1YR, how='outer', on = 'SK_ID_BUREAU')\n",
    "DPD_1YR = pd.merge(DPD_1YR, DPD4_1YR, how='outer', on = 'SK_ID_BUREAU')\n",
    "DPD_1YR = pd.merge(DPD_1YR, DPD5_1YR, how='outer', on = 'SK_ID_BUREAU')\n",
    "DPD_1YR = pd.merge(DPD_1YR, DPDany_1YR, how='outer', on = 'SK_ID_BUREAU')\n",
    "\n",
    "# No. of times a SK_ID_BUREAU had DPD 1, 2, 3, 4, 5 or any  overall\n",
    "DPD1_overall = bureau_balance[bureau_balance.STATUS == '1'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD1_overall.columns = ('SK_ID_BUREAU', 'DPD1_overall')\n",
    "\n",
    "DPD2_overall = bureau_balance[bureau_balance.STATUS == '2'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD2_overall.columns = ('SK_ID_BUREAU', 'DPD2_overall')\n",
    "\n",
    "DPD3_overall = bureau_balance[bureau_balance.STATUS == '3'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD3_overall.columns = ('SK_ID_BUREAU', 'DPD3_overall')\n",
    "\n",
    "DPD4_overall = bureau_balance[bureau_balance.STATUS == '4'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD4_overall.columns = ('SK_ID_BUREAU', 'DPD4_overall')\n",
    "\n",
    "DPD5_overall = bureau_balance[bureau_balance.STATUS == '5'].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPD5_overall.columns = ('SK_ID_BUREAU', 'DPD5_overall')\n",
    "\n",
    "DPDany_overall = bureau_balance[bureau_balance.STATUS.isin(['1','2','3','4','5'])].groupby('SK_ID_BUREAU', as_index=False).count()[['SK_ID_BUREAU', 'STATUS']]\n",
    "DPDany_overall.columns = ('SK_ID_BUREAU', 'DPDany_overall')\n",
    "\n",
    "DPD_overall = pd.merge(DPD1_overall, DPD2_overall, how='outer', on = 'SK_ID_BUREAU')\n",
    "DPD_overall = pd.merge(DPD_overall, DPD3_overall, how='outer', on = 'SK_ID_BUREAU')\n",
    "DPD_overall = pd.merge(DPD_overall, DPD4_overall, how='outer', on = 'SK_ID_BUREAU')\n",
    "DPD_overall = pd.merge(DPD_overall, DPD5_overall, how='outer', on = 'SK_ID_BUREAU')\n",
    "DPD_overall = pd.merge(DPD_overall, DPDany_overall, how='outer', on = 'SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_bal_new = pd.merge(DPD_1YR, DPD_overall, how='outer', on = 'SK_ID_BUREAU')\n",
    "bureau_bal_new = pd.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], bureau_bal_new, how='left', on = 'SK_ID_BUREAU')\n",
    "bureau_bal_new.iloc[:, 1:] = bureau_bal_new.iloc[:, 1:].apply(lambda x: (x - np.mean(x)) / np.std(x), axis = 0)\n",
    "bureau_bal_new = bureau_bal_new.fillna(0)\n",
    "bureau_bal_new = bureau_bal_new.drop('SK_ID_BUREAU', axis = 1).groupby('SK_ID_CURR', as_index = False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging bureau_bal_new with bureau_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_train_new = pd.merge(bureau_train_new, bureau_bal_new, on = 'SK_ID_CURR', how = 'left')\n",
    "bureau_test_new = pd.merge(bureau_test_new, bureau_bal_new, on = 'SK_ID_CURR', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging with main application dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_bur_train = pd.merge(app_train_mod, bureau_train_new, on = 'SK_ID_CURR', how = 'left')\n",
    "app_bur_test = pd.merge(app_test_mod, bureau_test_new, on = 'SK_ID_CURR', how = 'left')\n",
    "app_bur_train = app_bur_train.fillna(0)\n",
    "app_bur_test = app_bur_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_bur_train.to_csv('./data/app_bur_train.csv', index = False)\n",
    "# app_bur_test.to_csv('./data/app_bur_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "one_index = app_bur_train[app_bur_train.TARGET == 1].index\n",
    "zero_index = app_bur_train[app_bur_train.TARGET == 0].index\n",
    "\n",
    "trainindex1 = np.random.choice(one_index, size = int(0.7*one_index.shape[0]), replace = False)\n",
    "trainindex0 = np.random.choice(zero_index, size = int(0.7*zero_index.shape[0]), replace = False)\n",
    "trainindex = np.concatenate([trainindex1, trainindex0])\n",
    "testindex = np.delete(app_bur_train.index, trainindex)\n",
    "\n",
    "fit_df = app_bur_train.iloc[trainindex]\n",
    "validation_df = app_bur_train.iloc[testindex]\n",
    "\n",
    "X_fit = fit_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_fit = fit_df['TARGET']\n",
    "X_validation = validation_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_validation = validation_df['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 500    max_depth: 9    max_features: 12\n",
      "train_roc: 0.796    validation_roc: 0.746\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "rfmodel = ensemble.RandomForestClassifier(class_weight='balanced')\n",
    "param_grid = {'n_estimators': [500], 'max_depth':[9], 'max_features':[12]}\n",
    "gridmodel = model_selection.GridSearchCV(rfmodel, param_grid, scoring = 'roc_auc', cv = 10)\n",
    "gridmodel.fit(X_fit, y_fit)\n",
    "\n",
    "print('n_estimators:', gridmodel.best_estimator_.n_estimators, '  ',\n",
    "      'max_depth:', gridmodel.best_estimator_.max_depth, '  ',\n",
    "      'max_features:', gridmodel.best_estimator_.max_features)\n",
    "print('train_roc:', round(gridmodel.score(X_fit, y_fit), 3), '  ',\n",
    "      'validation_roc:', round(gridmodel.score(X_validation, y_validation), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73320651] [0.745017] [0.7526683] [0.74630336] [0.74101244] [0.74925354] [0.74814274] [0.74788384] [0.74706643] [0.74092569]\n"
     ]
    }
   ],
   "source": [
    "print(gridmodel.cv_results_['split0_test_score'], gridmodel.cv_results_['split1_test_score'], \n",
    "      gridmodel.cv_results_['split2_test_score'], gridmodel.cv_results_['split3_test_score'],\n",
    "      gridmodel.cv_results_['split4_test_score'], gridmodel.cv_results_['split5_test_score'],\n",
    "     gridmodel.cv_results_['split6_test_score'], gridmodel.cv_results_['split7_test_score'],\n",
    "     gridmodel.cv_results_['split8_test_score'], gridmodel.cv_results_['split9_test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle\n",
    "X_train = app_bur_train.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_train = app_bur_train.TARGET\n",
    "X_test = app_bur_test.drop('SK_ID_CURR', axis = 1)\n",
    "\n",
    "# Model fit and prediction\n",
    "model = ensemble.RandomForestClassifier(n_estimators = 200, max_depth = 3, max_features = 4, class_weight = 'balanced')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = pd.DataFrame({'SK_ID_CURR': app_bur_test.SK_ID_CURR, 'TARGET': y_pred[:,1]})\n",
    "sol.to_csv('./soln/sol_bureaurf.csv', index = False)                   # kaggle: 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample \n",
    "xs = app_bur_train.drop('SK_ID_CURR', axis = 1)\n",
    "ys = app_bur_train.TARGET\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = model_selection.train_test_split(xs, ys,\n",
    "                                                                                test_size=0.30, random_state=0, \n",
    "                                                                                stratify=app_bur_train.TARGET)\n",
    "X_train_0 = X_train[X_train.TARGET == 0]\n",
    "X_train_1 = X_train[X_train.TARGET == 1]\n",
    "X_train_1 = X_train_1.sample(X_train_0.shape[0], replace=True)\n",
    "X_train = pd.concat([X_train_0, X_train_1], axis = 0)\n",
    "y_train = X_train.TARGET\n",
    "X_train = X_train.drop('TARGET', axis = 1)\n",
    "X_validation = X_validation.drop('TARGET', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=0.7, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "n_estimators: 200    learning_rate: 0.05    max_depth: 6\n",
      "train_roc: 0.832    validation_roc: 0.766\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>0.821752</td>\n",
       "      <td>0.834573</td>\n",
       "      <td>0.000731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_n_estimators param_learning_rate param_max_depth  mean_test_score  \\\n",
       "0                200                0.05               6         0.821752   \n",
       "\n",
       "   mean_train_score  std_test_score  \n",
       "0          0.834573        0.000731  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "sgbmodel = ensemble.GradientBoostingClassifier(subsample=0.7)\n",
    "param_grid = {\"n_estimators\": [200], \"learning_rate\": [0.05], \"max_depth\": [6]}\n",
    "gridmodel = model_selection.GridSearchCV(sgbmodel, param_grid, scoring = 'roc_auc')\n",
    "gridmodel.fit(X_train, y_train)\n",
    "\n",
    "print(gridmodel.best_estimator_)\n",
    "print('n_estimators:', gridmodel.best_estimator_.n_estimators, '  ',\n",
    "      'learning_rate:', gridmodel.best_estimator_.learning_rate, '  ',\n",
    "      'max_depth:', gridmodel.best_estimator_.max_depth)\n",
    "print('train_roc:', round(gridmodel.score(X_train, y_train), 3), '  ',\n",
    "      'validation_roc:', round(gridmodel.score(X_validation, y_validation), 3))\n",
    "\n",
    "sgb = pd.DataFrame(gridmodel.cv_results_)[['param_n_estimators', 'param_learning_rate', 'param_max_depth', \n",
    "                                      'mean_test_score', 'mean_train_score', 'std_test_score']]\n",
    "try:\n",
    "    sgb_results = pd.concat([sgb_results, sgb])\n",
    "except:\n",
    "    sgb_results = sgb.copy()\n",
    "sgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>...</th>\n",
       "      <th>DPD3_1YR</th>\n",
       "      <th>DPD4_1YR</th>\n",
       "      <th>DPD5_1YR</th>\n",
       "      <th>DPDany_1YR</th>\n",
       "      <th>DPD1_overall</th>\n",
       "      <th>DPD2_overall</th>\n",
       "      <th>DPD3_overall</th>\n",
       "      <th>DPD4_overall</th>\n",
       "      <th>DPD5_overall</th>\n",
       "      <th>DPDany_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>1.180505</td>\n",
       "      <td>1.725450</td>\n",
       "      <td>0.592683</td>\n",
       "      <td>1.600873</td>\n",
       "      <td>-0.146313</td>\n",
       "      <td>-0.499013</td>\n",
       "      <td>1.599337</td>\n",
       "      <td>-2.051813</td>\n",
       "      <td>-0.291208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>-1.133470</td>\n",
       "      <td>-1.152888</td>\n",
       "      <td>-1.404669</td>\n",
       "      <td>-1.092145</td>\n",
       "      <td>2.242932</td>\n",
       "      <td>2.003956</td>\n",
       "      <td>1.599337</td>\n",
       "      <td>-0.062699</td>\n",
       "      <td>-0.291208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "1      100003          1.180505    1.725450     0.592683         1.600873   \n",
       "2      100004         -1.133470   -1.152888    -1.404669        -1.092145   \n",
       "\n",
       "   OWN_CAR_AGE  FLAG_WORK_PHONE  FLAG_PHONE  REGION_RATING_CLIENT_W_CITY  \\\n",
       "1    -0.146313        -0.499013    1.599337                    -2.051813   \n",
       "2     2.242932         2.003956    1.599337                    -0.062699   \n",
       "\n",
       "   REG_CITY_NOT_LIVE_CITY       ...        DPD3_1YR  DPD4_1YR  DPD5_1YR  \\\n",
       "1               -0.291208       ...             0.0       0.0       0.0   \n",
       "2               -0.291208       ...             0.0       0.0       0.0   \n",
       "\n",
       "   DPDany_1YR  DPD1_overall  DPD2_overall  DPD3_overall  DPD4_overall  \\\n",
       "1         0.0           0.0           0.0           0.0           0.0   \n",
       "2         0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   DPD5_overall  DPDany_overall  \n",
       "1           0.0             0.0  \n",
       "2           0.0             0.0  \n",
       "\n",
       "[2 rows x 92 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>...</th>\n",
       "      <th>DPD3_1YR</th>\n",
       "      <th>DPD4_1YR</th>\n",
       "      <th>DPD5_1YR</th>\n",
       "      <th>DPDany_1YR</th>\n",
       "      <th>DPD1_overall</th>\n",
       "      <th>DPD2_overall</th>\n",
       "      <th>DPD3_overall</th>\n",
       "      <th>DPD4_overall</th>\n",
       "      <th>DPD5_overall</th>\n",
       "      <th>DPDany_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.427809</td>\n",
       "      <td>0.142475</td>\n",
       "      <td>-0.553580</td>\n",
       "      <td>-0.037477</td>\n",
       "      <td>-0.138436</td>\n",
       "      <td>-0.507337</td>\n",
       "      <td>-0.597571</td>\n",
       "      <td>-0.024421</td>\n",
       "      <td>-0.289777</td>\n",
       "      <td>1.595754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.782413</td>\n",
       "      <td>-0.804537</td>\n",
       "      <td>-0.752831</td>\n",
       "      <td>-0.839362</td>\n",
       "      <td>-0.138436</td>\n",
       "      <td>-0.507337</td>\n",
       "      <td>-0.597571</td>\n",
       "      <td>-0.024421</td>\n",
       "      <td>-0.289777</td>\n",
       "      <td>0.393719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  OWN_CAR_AGE  \\\n",
       "0         -0.427809    0.142475    -0.553580        -0.037477    -0.138436   \n",
       "1         -0.782413   -0.804537    -0.752831        -0.839362    -0.138436   \n",
       "\n",
       "   FLAG_WORK_PHONE  FLAG_PHONE  REGION_RATING_CLIENT_W_CITY  \\\n",
       "0        -0.507337   -0.597571                    -0.024421   \n",
       "1        -0.507337   -0.597571                    -0.024421   \n",
       "\n",
       "   REG_CITY_NOT_LIVE_CITY  EXT_SOURCE_1       ...        DPD3_1YR  DPD4_1YR  \\\n",
       "0               -0.289777      1.595754       ...             0.0       0.0   \n",
       "1               -0.289777      0.393719       ...             0.0       0.0   \n",
       "\n",
       "   DPD5_1YR  DPDany_1YR  DPD1_overall  DPD2_overall  DPD3_overall  \\\n",
       "0       0.0         1.0           1.0           0.0           0.0   \n",
       "1       0.0         0.0           0.0           0.0           0.0   \n",
       "\n",
       "   DPD4_overall  DPD5_overall  DPDany_overall  \n",
       "0           0.0           0.0             1.0  \n",
       "1           0.0           0.0             0.0  \n",
       "\n",
       "[2 rows x 91 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle STOCHASTIC GRADIENT BOOSTING\n",
    "# upscale\n",
    "X_train = app_bur_train.copy()\n",
    "X_train_0 = X_train[X_train.TARGET == 0]\n",
    "X_train_1 = X_train[X_train.TARGET == 1]\n",
    "X_train_1 = X_train_1.sample(X_train_0.shape[0], replace=True)\n",
    "X_train = pd.concat([X_train_0, X_train_1], axis = 0)\n",
    "y_train = X_train.TARGET\n",
    "\n",
    "X_train = X_train.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "X_test = app_bur_test.drop('SK_ID_CURR', axis = 1)\n",
    "\n",
    "\n",
    "# Model fit and prediction\n",
    "model = ensemble.GradientBoostingClassifier(n_estimators = 200, learning_rate = 0.05, max_depth = 6, subsample=0.7)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = pd.DataFrame({'SK_ID_CURR': app_bur_test.SK_ID_CURR, 'TARGET': y_pred[:,1]})\n",
    "sol.to_csv('./soln/sol_bureaugb.csv', index = False)                   # kaggle: 0.72664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
