{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import impute\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = pd.read_csv('./data/application_train.csv')\n",
    "application_test = pd.read_csv('./data/application_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train.replace('XNA', np.NaN, inplace = True)\n",
    "application_train['age_yrs'] = np.negative(application_train['DAYS_BIRTH'])/365\n",
    "application_train['yrs_emp'] = np.negative(application_train['DAYS_EMPLOYED'])/365\n",
    "application_train['yrs_registration'] = np.negative(application_train['DAYS_REGISTRATION'])/365\n",
    "application_train['yrs_id_publish'] = np.negative(application_train['DAYS_ID_PUBLISH'])/365\n",
    "application_train['yrs_last_phone_change'] = np.negative(application_train['DAYS_LAST_PHONE_CHANGE'])/365\n",
    "application_train = application_train.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE'], axis = 1)\n",
    "\n",
    "application_test.replace('XNA', np.NaN, inplace = True)\n",
    "application_test['age_yrs'] = np.negative(application_test['DAYS_BIRTH'])/365\n",
    "application_test['yrs_emp'] = np.negative(application_test['DAYS_EMPLOYED'])/365\n",
    "application_test['yrs_registration'] = np.negative(application_test['DAYS_REGISTRATION'])/365\n",
    "application_test['yrs_id_publish'] = np.negative(application_test['DAYS_ID_PUBLISH'])/365\n",
    "application_test['yrs_last_phone_change'] = np.negative(application_test['DAYS_LAST_PHONE_CHANGE'])/365\n",
    "application_test = application_test.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_anomalies = application_train[(application_train.AMT_INCOME_TOTAL - np.mean(application_train.AMT_INCOME_TOTAL))/np.std(application_train.AMT_INCOME_TOTAL) > 3]\n",
    "application_train.AMT_INCOME_TOTAL.replace(inc_anomalies.AMT_INCOME_TOTAL.values, np.mean(application_train.AMT_INCOME_TOTAL), inplace = True)\n",
    "\n",
    "# replacing one aberration with mean\n",
    "application_train.OBS_30_CNT_SOCIAL_CIRCLE.replace(max(application_train.OBS_30_CNT_SOCIAL_CIRCLE), np.mean(application_train.OBS_30_CNT_SOCIAL_CIRCLE), inplace = True)\n",
    "application_train.DEF_30_CNT_SOCIAL_CIRCLE.replace(max(application_train.DEF_30_CNT_SOCIAL_CIRCLE), np.mean(application_train.DEF_30_CNT_SOCIAL_CIRCLE), inplace = True)\n",
    "application_train.OBS_60_CNT_SOCIAL_CIRCLE.replace(max(application_train.OBS_60_CNT_SOCIAL_CIRCLE), np.mean(application_train.OBS_60_CNT_SOCIAL_CIRCLE), inplace = True)\n",
    "application_train.DEF_60_CNT_SOCIAL_CIRCLE.replace(max(application_train.DEF_60_CNT_SOCIAL_CIRCLE), np.mean(application_train.DEF_60_CNT_SOCIAL_CIRCLE), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing numerical variables in train\n",
    "numerical_var = application_train.dtypes[application_train.dtypes != 'object'].drop(['SK_ID_CURR', 'TARGET'])\n",
    "application_train[numerical_var.index] = application_train[numerical_var.index].apply(lambda x: (x - np.nanmean(x)) / np.nanstd(x), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of missing in categorical variables\n",
    "categorical_var = application_train.dtypes[application_train.dtypes == 'object']\n",
    "cat_df = application_train[categorical_var.index]\n",
    "cat_missing = pd.isnull(cat_df).sum(axis = 0)[pd.isnull(cat_df).sum(axis = 0) > 0]/cat_df.shape[0]\n",
    "\n",
    "# proportion of missing in numerical variables\n",
    "numerical_var = application_train.dtypes[application_train.dtypes != 'object']\n",
    "num_df = application_train[numerical_var.index]\n",
    "num_missing = pd.isnull(num_df).sum(axis = 0)[pd.isnull(num_df).sum(axis = 0) > 0]/num_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train1 = application_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with First Missingness Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.687298105792378, 0.6884280174825237, 0.6880477688578055, 0.6879875363900797, 0.6880258484948221]\n",
      "[0.6751383102390284, 0.6755991221944275, 0.6766814100739129, 0.6764446388703472, 0.6764845701542155]\n"
     ]
    }
   ],
   "source": [
    "# Imputation\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'most_frequent')\n",
    "application_train[cat_missing.index] = imp.fit_transform(application_train[cat_missing.index])\n",
    "\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'mean')\n",
    "application_train[num_missing.index] = imp.fit_transform(application_train[num_missing.index])\n",
    "\n",
    "# Dummification of categorical variables\n",
    "logistic_train = application_train.copy()\n",
    "logistic_test = application_test.copy()\n",
    "\n",
    "dummy_train = logistic_train[categorical_var.index]\n",
    "dummy_test = logistic_test[categorical_var.index]\n",
    "\n",
    "dummy_train['code'] = 1\n",
    "dummy_test['code'] = 0\n",
    "\n",
    "dummy_joined = pd.concat([dummy_train, dummy_test])\n",
    "dummy_joined = pd.get_dummies(dummy_joined, drop_first = True)\n",
    "\n",
    "dummy_train = dummy_joined[dummy_joined.code == 1]\n",
    "dummy_test = dummy_joined[dummy_joined.code == 0]\n",
    "\n",
    "logistic_train = logistic_train.drop(categorical_var.index, axis=1)\n",
    "logistic_test = logistic_test.drop(categorical_var.index, axis=1)\n",
    "\n",
    "logistic_train = pd.concat([logistic_train, dummy_train], axis = 1)\n",
    "logistic_test = pd.concat([logistic_test, dummy_test], axis = 1)\n",
    "\n",
    "# Train-Test Split\n",
    "one_index = logistic_train[logistic_train.TARGET == 1].index\n",
    "zero_index = logistic_train[logistic_train.TARGET == 0].index\n",
    "\n",
    "trainindex1 = np.random.choice(one_index, size = int(0.7*one_index.shape[0]), replace = False)\n",
    "trainindex0 = np.random.choice(zero_index, size = int(0.7*zero_index.shape[0]), replace = False)\n",
    "trainindex = np.concatenate([trainindex1, trainindex0])\n",
    "testindex = np.delete(logistic_train.index, trainindex)\n",
    "\n",
    "fit_df = logistic_train.iloc[trainindex]\n",
    "validation_df = logistic_train.iloc[testindex]\n",
    "\n",
    "X_fit = fit_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_fit = fit_df['TARGET']\n",
    "X_validation = validation_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_validation = validation_df['TARGET']\n",
    "\n",
    "# Model fit and prediction\n",
    "roc_fit = []\n",
    "roc_validation = []\n",
    "for c in [0.01, 0.1, 1, 10, 1000]:\n",
    "    logitmodel = linear_model.LogisticRegression(C = c, class_weight = 'balanced')\n",
    "    logitmodel.fit(X_fit, y_fit)\n",
    "\n",
    "    y_pred_fit = logitmodel.predict(X_fit)\n",
    "    roc_fit.append(metrics.roc_auc_score(y_fit, y_pred_fit))\n",
    "\n",
    "    y_pred_validation = logitmodel.predict(X_validation)\n",
    "    roc_validation.append(metrics.roc_auc_score(y_validation, y_pred_validation))\n",
    "print(roc_fit)\n",
    "print(roc_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Second Missingness Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6855309695036478, 0.6856081624103685]\n",
      "[0.6831293102576974, 0.6829760192169692]\n"
     ]
    }
   ],
   "source": [
    "application_train = application_train1.copy()\n",
    "\n",
    "# Missingness imputation in train\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'constant', fill_value = 'xan')\n",
    "application_train[cat_missing.index] = imp.fit_transform(application_train[cat_missing.index])\n",
    "\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'mean')\n",
    "application_train[num_missing.index] = imp.fit_transform(application_train[num_missing.index])\n",
    "\n",
    "# Missingness imputation in test\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'constant', fill_value = 'xan')\n",
    "application_test[cat_missing.index] = imp.fit_transform(application_test[cat_missing.index])\n",
    "\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'mean')\n",
    "application_test[num_missing.index] = imp.fit_transform(application_test[num_missing.index])\n",
    "\n",
    "# Dummification of categorical variables\n",
    "logistic_train = application_train.copy()\n",
    "logistic_test = application_test.copy()\n",
    "\n",
    "dummy_train = logistic_train[categorical_var.index]\n",
    "dummy_test = logistic_test[categorical_var.index]\n",
    "\n",
    "dummy_train['code'] = 1\n",
    "dummy_test['code'] = 0\n",
    "\n",
    "dummy_joined = pd.concat([dummy_train, dummy_test])\n",
    "dummy_joined = pd.get_dummies(dummy_joined, drop_first = True)\n",
    "\n",
    "dummy_train = dummy_joined[dummy_joined.code == 1]\n",
    "dummy_test = dummy_joined[dummy_joined.code == 0]\n",
    "\n",
    "logistic_train = logistic_train.drop(categorical_var.index, axis=1)\n",
    "logistic_test = logistic_test.drop(categorical_var.index, axis=1)\n",
    "\n",
    "logistic_train = pd.concat([logistic_train, dummy_train], axis = 1)\n",
    "logistic_test = pd.concat([logistic_test, dummy_test], axis = 1)\n",
    "\n",
    "# Train-Test Split\n",
    "one_index = logistic_train[logistic_train.TARGET == 1].index\n",
    "zero_index = logistic_train[logistic_train.TARGET == 0].index\n",
    "\n",
    "trainindex1 = np.random.choice(one_index, size = int(0.7*one_index.shape[0]), replace = False)\n",
    "trainindex0 = np.random.choice(zero_index, size = int(0.7*zero_index.shape[0]), replace = False)\n",
    "trainindex = np.concatenate([trainindex1, trainindex0])\n",
    "testindex = np.delete(logistic_train.index, trainindex)\n",
    "\n",
    "fit_df = logistic_train.iloc[trainindex]\n",
    "validation_df = logistic_train.iloc[testindex]\n",
    "\n",
    "X_fit = fit_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_fit = fit_df['TARGET']\n",
    "X_validation = validation_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_validation = validation_df['TARGET']\n",
    "\n",
    "# Model fit and prediction\n",
    "roc_fit = []\n",
    "roc_validation = []\n",
    "for c in [1, 10]:\n",
    "    logitmodel = linear_model.LogisticRegression(C = c, class_weight = 'balanced')\n",
    "    logitmodel.fit(X_fit, y_fit)\n",
    "\n",
    "    y_pred_fit = logitmodel.predict(X_fit)\n",
    "    roc_fit.append(metrics.roc_auc_score(y_fit, y_pred_fit))\n",
    "\n",
    "    y_pred_validation = logitmodel.predict(X_validation)\n",
    "    roc_validation.append(metrics.roc_auc_score(y_validation, y_pred_validation))\n",
    "print(roc_fit)\n",
    "print(roc_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Third Missingness Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6817085567509356, 0.6817108315398795, 0.6819721629697187]\n",
      "[0.6872708597847876, 0.6889128489961278, 0.6891514978661224]\n"
     ]
    }
   ],
   "source": [
    "application_train = application_train1.copy()\n",
    "\n",
    "# # Imputation\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'constant', fill_value = 'xan')\n",
    "application_train[cat_missing.index] = imp.fit_transform(application_train[cat_missing.index])\n",
    "\n",
    "imp = impute.SimpleImputer(missing_values=np.NaN, strategy= 'median')\n",
    "application_train[num_missing.index] = imp.fit_transform(application_train[num_missing.index])\n",
    "\n",
    "# pd.isnull(application_train).sum(axis = 0)[pd.isnull(application_train).sum(axis = 0) > 0]/application_train.shape[0]\n",
    "\n",
    "# # Dummification of categorical variables\n",
    "logistic_train = application_train.copy()\n",
    "logistic_test = application_test.copy()\n",
    "\n",
    "dummy_train = logistic_train[categorical_var.index]\n",
    "dummy_test = logistic_test[categorical_var.index]\n",
    "\n",
    "dummy_train['code'] = 1\n",
    "dummy_test['code'] = 0\n",
    "\n",
    "dummy_joined = pd.concat([dummy_train, dummy_test])\n",
    "dummy_joined = pd.get_dummies(dummy_joined, drop_first = True)\n",
    "\n",
    "dummy_train = dummy_joined[dummy_joined.code == 1]\n",
    "dummy_test = dummy_joined[dummy_joined.code == 0]\n",
    "\n",
    "logistic_train = logistic_train.drop(categorical_var.index, axis=1)\n",
    "logistic_test = logistic_test.drop(categorical_var.index, axis=1)\n",
    "\n",
    "logistic_train = pd.concat([logistic_train, dummy_train], axis = 1)\n",
    "logistic_test = pd.concat([logistic_test, dummy_test], axis = 1)\n",
    "\n",
    "# # Train-Test Split\n",
    "one_index = logistic_train[logistic_train.TARGET == 1].index\n",
    "zero_index = logistic_train[logistic_train.TARGET == 0].index\n",
    "\n",
    "trainindex1 = np.random.choice(one_index, size = int(0.7*one_index.shape[0]), replace = False)\n",
    "trainindex0 = np.random.choice(zero_index, size = int(0.7*zero_index.shape[0]), replace = False)\n",
    "trainindex = np.concatenate([trainindex1, trainindex0])\n",
    "testindex = np.delete(logistic_train.index, trainindex)\n",
    "\n",
    "fit_df = logistic_train.iloc[trainindex]\n",
    "validation_df = logistic_train.iloc[testindex]\n",
    "\n",
    "X_fit = fit_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_fit = fit_df['TARGET']\n",
    "X_validation = validation_df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_validation = validation_df['TARGET']\n",
    "\n",
    "# # Model fit and prediction\n",
    "roc_fit = []\n",
    "roc_validation = []\n",
    "for c in [0.01, 1, 1000]:\n",
    "    logitmodel = linear_model.LogisticRegression(C = c, class_weight = 'balanced')\n",
    "    logitmodel.fit(X_fit, y_fit)\n",
    "\n",
    "    y_pred_fit = logitmodel.predict(X_fit)\n",
    "    roc_fit.append(metrics.roc_auc_score(y_fit, y_pred_fit))\n",
    "\n",
    "    y_pred_validation = logitmodel.predict(X_validation)\n",
    "    roc_validation.append(metrics.roc_auc_score(y_validation, y_pred_validation))\n",
    "print(roc_fit)\n",
    "print(roc_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
